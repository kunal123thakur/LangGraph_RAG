![alt text](<Screenshot 2025-03-27 114723.png>)


Hereâ€™s an enhanced version of your `README.md` with better formatting, sections, and styling to make it visually appealing and informative.

```markdown
# ğŸ” Corrective RAG LangGraph Application

A simple Streamlit application demonstrating **Retrieval-Augmented Generation (RAG)** using the **LangGraph** framework. The app integrates **Groq**, **Google Generative AI**, **Chroma**, and **LangChain** to build a powerful RAG pipeline that answers user queries based on document content.

---

## ğŸŒŸ Features

- **Retrieval-Augmented Generation (RAG)**: Combines document retrieval and language generation for accurate responses.
- **Groq LLM Integration**: Uses **ChatGroq** for generating brief, context-based answers.
- **Google Generative AI Embeddings**: Embeds documents for efficient search and retrieval.
- **Streamlit UI**: User-friendly interface for interactive query submission and result display.

---

## ğŸ“‹ Requirements

Ensure you have the following:

- **Python 3.7+** (preferably within a virtual environment)
- **Streamlit**: To run the web app
- **Groq API Key**: Required to use the Groq LLM
- **Google API Key**: Needed for Generative AI embeddings

### Install Dependencies

Run the following command to install the required dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/rag-langgraph.git
cd rag-langgraph
```

### 2. Set Up Environment Variables

Create a `.env` file in the root directory and add your API keys for **Groq** and **Google Generative AI**:

```env
GROK_API_KEY=your_grok_api_key
GOOGLE_API_KEY=your_google_api_key
```

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ app.py                  # Main app file that runs the RAG pipeline
â”œâ”€â”€ data/                   # Directory containing .txt documents for retrieval
â”‚   â””â”€â”€ *.txt               # Your text files for querying
â”œâ”€â”€ requirements.txt        # Required dependencies
â””â”€â”€ .env                    # Environment variables for API keys
```

---

## ğŸš€ How to Run the Application

1. **Install Dependencies**:

   If you haven't already, run the following to install required packages:

   ```bash
   pip install -r requirements.txt
   ```

2. **Start the App**:

   Once the dependencies are installed and `.env` is configured, run:

   ```bash
   streamlit run app.py
   ```

   The app will be live at `http://localhost:8501`.

---

## ğŸ“ How It Works

1. **Document Loading & Splitting**:
   - Text documents from the `data/` folder are loaded and split into smaller chunks for efficient retrieval.

2. **Embedding & Document Retrieval**:
   - **Google Generative AI** is used to generate embeddings for the documents.
   - **Chroma** vector store enables fast, efficient document retrieval based on the userâ€™s query.

3. **LangGraph Workflow**:
   - **LangGraph** orchestrates the retrieval and generation steps through a simple workflow:
     - `LLM` node generates a brief answer to the query.
     - `RAGtool` node refines the answer using context retrieved from documents.

4. **Streamlit Interface**:
   - A user enters a question.
   - The app triggers the LangGraph workflow, retrieves the relevant documents, and generates an answer, all presented in an easy-to-read format.

---

## ğŸ”§ Workflow Overview

- **`function_1` (LLM Node)**: Processes the user's question and generates a short answer.
- **`function_2` (RAGtool Node)**: Retrieves relevant documents based on the question, and combines the context to generate a detailed response.

---

## ğŸ® Example

1. **User Input**: "What is the main idea of the document?"
2. **Run the RAG Pipeline**: Press the **"Run RAG Pipeline"** button.
3. **Output**: The app will display the answer generated by combining document context and Groq's language model.

---

## ğŸ“Š UI Interface

The Streamlit UI features the following:

- **Text Input**: Enter your query about the documents.
- **Run Button**: Starts the RAG pipeline.
- **Results Display**: Outputs the response generated by the model.

---

## ğŸ“ Example Interaction

1. **User asks**: "What is the significance of RAG?"
2. **App Output**: 

   > **ğŸ“Œ Output from LLM**:  
   "RAG combines document retrieval with language generation, enhancing response accuracy."
   
   > **ğŸ“Œ Output from RAGtool**:  
   "RAG helps in providing more contextually accurate responses by augmenting generated answers with retrieved document information."

---

## âš–ï¸ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

We welcome contributions to improve the project. Feel free to:

1. Fork the repo
2. Open an issue or submit a pull request

---

## ğŸ’¬ Questions?

For any questions or issues, please open an issue or contact the project maintainers. Happy coding! ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸ’»

```

---

### What's New in This Version?

1. **Sections**: Clearer section headers (`Features`, `Requirements`, `How It Works`, etc.).
2. **Emojis**: Added emojis to make it more visually appealing.
3. **Improved Layout**: Clean separation of sections, with each one explaining a distinct part of the app.
4. **Workflow**: Added a dedicated "Workflow Overview" section to explain the LangGraph-based pipeline.
5. **UI Description**: Clearly explains the interaction between the user and the app.

This layout should make it easier for users to understand how to use and set up the application! Let me know if you need further tweaks.